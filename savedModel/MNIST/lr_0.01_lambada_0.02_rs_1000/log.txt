Learning Rate	Train Loss	Valid Loss	Train Acc.	Valid Acc.	loss normal	loss gradient	
{'adv_only': False, 'checkpoint': None, 'ckpt_interval': 10, 'dataset': 'MNIST', 'debug_mode': False, 'drop': 0, 'epochs': 100, 'epsilon': 0.3, 'evaluate': False, 'gamma': 0.1, 'gpu_id': '2', 'lambada': 0.02, 'lr': 0.01, 'manualSeed': 1000, 'method': 'LFM', 'momentum': 0.9, 'mu': 1.0, 'norm': 'l_inf', 'num_steps': 40, 'opt_g_gamma': 0.1, 'opt_g_lr': 0.1, 'opt_g_schedule': [100, 150, 200], 'opt_g_weight_decay': 0.0002, 'resume': '', 'save_dir': '0503dc_mnist', 'schedule': [90], 'start_epoch': 0, 'step_size': 0.01, 'test_batch': 128, 'train_batch': 128, 'weight_decay': 0, 'workers': 4}
0.010000	0.929111	0.203662	82.570000	96.450000	0.632717	14.819698	
0.010000	0.500102	0.179113	96.416667	96.660000	0.197830	15.113630	
0.010000	0.449688	0.142523	97.165000	97.720000	0.163744	14.297229	
0.010000	0.423701	0.129906	97.540000	97.910000	0.148273	13.771392	
0.010000	0.408440	0.126880	97.711667	98.170000	0.138562	13.493869	
0.010000	0.393337	0.115656	97.931667	98.200000	0.128667	13.233520	
0.010000	0.382955	0.135735	97.963333	97.580000	0.122876	13.003975	
0.010000	0.370962	0.103912	98.115000	98.520000	0.115767	12.759720	
0.010000	0.363040	0.110141	98.190000	97.990000	0.111487	12.577671	
0.010000	0.354921	0.108455	98.281667	97.980000	0.106255	12.433274	
0.010000	0.348622	0.103310	98.288333	98.310000	0.103540	12.254074	
0.010000	0.342339	0.093974	98.350000	98.450000	0.100024	12.115723	
0.010000	0.335522	0.093794	98.398333	98.480000	0.096489	11.951661	
0.010000	0.329744	0.087803	98.428333	98.480000	0.093960	11.789214	
0.010000	0.324628	0.093876	98.453333	98.140000	0.091321	11.665346	
0.010000	0.320957	0.085870	98.516667	98.560000	0.089073	11.594217	
0.010000	0.316721	0.081800	98.476667	98.710000	0.087365	11.467829	
0.010000	0.312314	0.081068	98.556667	98.590000	0.085508	11.340312	
0.010000	0.308461	0.076181	98.500000	98.610000	0.083791	11.233493	
0.010000	0.302784	0.081663	98.596667	98.560000	0.081466	11.065894	
0.010000	0.297113	0.078767	98.608333	98.520000	0.078545	10.928441	
0.010000	0.293938	0.075484	98.618333	98.560000	0.077577	10.818038	
0.010000	0.286270	0.068564	98.660000	98.740000	0.074249	10.601041	
0.010000	0.282848	0.076708	98.700000	98.380000	0.072819	10.501477	
0.010000	0.277815	0.074838	98.690000	98.360000	0.071049	10.338285	
0.010000	0.275817	0.073395	98.731667	98.620000	0.070629	10.259386	
0.010000	0.272159	0.079477	98.725000	98.270000	0.068982	10.158831	
0.010000	0.268328	0.067252	98.743333	98.700000	0.068110	10.010884	
0.010000	0.263994	0.064887	98.750000	98.710000	0.066437	9.877812	
0.010000	0.262026	0.061723	98.735000	98.800000	0.066133	9.794651	
0.010000	0.257525	0.064809	98.780000	98.630000	0.064411	9.655679	
0.010000	0.254484	0.058605	98.775000	98.740000	0.062884	9.579997	
0.010000	0.249448	0.063012	98.796667	98.630000	0.060886	9.428133	
0.010000	0.246647	0.055479	98.828333	98.850000	0.060483	9.308229	
0.010000	0.240651	0.054871	98.825000	98.730000	0.059022	9.081423	
0.010000	0.231230	0.054331	98.845000	98.650000	0.056241	8.749473	
0.010000	0.215109	0.053269	98.851667	98.820000	0.052144	8.148239	
0.010000	0.195392	0.042898	98.878333	98.970000	0.047987	7.370238	
0.010000	0.172848	0.040262	98.956667	98.870000	0.043332	6.475804	
0.010000	0.150615	0.041957	99.001667	98.800000	0.038854	5.588097	
0.010000	0.132260	0.035205	99.080000	98.990000	0.035193	4.853350	
0.010000	0.120193	0.029994	99.138333	99.070000	0.033120	4.353669	
0.010000	0.109805	0.034433	99.175000	98.920000	0.030006	3.989928	
0.010000	0.101959	0.030333	99.190000	99.090000	0.028264	3.684720	
0.010000	0.093519	0.027809	99.256667	99.260000	0.025997	3.376094	
0.010000	0.090845	0.029070	99.278333	99.180000	0.025759	3.254326	
0.010000	0.084160	0.025469	99.313333	99.200000	0.023206	3.047730	
0.010000	0.080315	0.028579	99.375000	99.160000	0.022231	2.904196	
0.010000	0.077089	0.024823	99.403333	99.230000	0.021090	2.799950	
0.010000	0.073590	0.024759	99.466667	99.190000	0.019636	2.697708	
0.010000	0.072128	0.026566	99.481667	99.090000	0.018972	2.657811	
0.010000	0.068969	0.024316	99.498333	99.300000	0.017860	2.555418	
0.010000	0.067616	0.023728	99.516667	99.240000	0.017499	2.505849	
0.010000	0.065748	0.022604	99.518333	99.260000	0.017114	2.431714	
0.010000	0.063408	0.023621	99.561667	99.270000	0.016103	2.365273	
0.010000	0.062285	0.025150	99.558333	99.220000	0.015768	2.325859	
0.010000	0.059605	0.024287	99.608333	99.230000	0.014459	2.257318	
0.010000	0.058875	0.022654	99.620000	99.210000	0.014066	2.240471	
0.010000	0.059119	0.021728	99.635000	99.320000	0.013806	2.265690	
0.010000	0.059429	0.021245	99.623333	99.300000	0.013836	2.279656	
0.010000	0.057864	0.022356	99.663333	99.310000	0.013287	2.228834	
0.010000	0.056810	0.020157	99.650000	99.370000	0.013088	2.186055	
0.010000	0.057160	0.020169	99.631667	99.290000	0.012970	2.209503	
0.010000	0.055160	0.020160	99.676667	99.360000	0.012167	2.149647	
0.010000	0.054694	0.020288	99.678333	99.370000	0.012075	2.130955	
0.010000	0.053823	0.019784	99.685000	99.360000	0.011799	2.101213	
0.010000	0.051581	0.021824	99.710000	99.310000	0.011339	2.012094	
0.010000	0.053007	0.021219	99.696667	99.260000	0.011602	2.070277	
0.010000	0.051322	0.021372	99.726667	99.350000	0.011012	2.015535	
0.010000	0.051327	0.021462	99.736667	99.320000	0.011023	2.015197	
0.010000	0.050341	0.025621	99.708333	99.180000	0.011348	1.949616	
0.010000	0.050499	0.022634	99.706667	99.300000	0.010570	1.996451	
0.010000	0.048930	0.019479	99.770000	99.380000	0.010017	1.945655	
0.010000	0.048767	0.020219	99.770000	99.290000	0.009816	1.947540	
0.010000	0.047932	0.022529	99.733333	99.200000	0.009878	1.902711	
0.010000	0.047406	0.023723	99.725000	99.250000	0.009916	1.874524	
0.010000	0.047132	0.018581	99.758333	99.340000	0.009415	1.885857	
0.010000	0.045911	0.019573	99.775000	99.340000	0.009356	1.827774	
0.010000	0.045606	0.018067	99.770000	99.390000	0.009102	1.825214	
0.010000	0.043397	0.019946	99.798333	99.390000	0.008423	1.748680	
0.010000	0.042967	0.020210	99.808333	99.350000	0.008113	1.742682	
0.010000	0.041534	0.018795	99.816667	99.390000	0.008207	1.666381	
0.010000	0.040931	0.019172	99.806667	99.260000	0.008127	1.640215	
0.010000	0.041839	0.017923	99.811667	99.370000	0.007908	1.696534	
0.010000	0.041018	0.017397	99.805000	99.390000	0.007889	1.656428	
0.010000	0.040672	0.019497	99.835000	99.350000	0.007494	1.658881	
0.010000	0.040015	0.017276	99.813333	99.380000	0.007404	1.630559	
0.010000	0.042620	0.021310	99.815000	99.320000	0.007941	1.733994	
0.010000	0.040664	0.018039	99.840000	99.400000	0.007196	1.673396	
0.010000	0.040699	0.018847	99.831667	99.310000	0.007241	1.672879	
state['lr']: 
0.001
0.001000	0.035056	0.016435	99.906667	99.490000	0.005147	1.495450	
0.001000	0.033275	0.016247	99.926667	99.500000	0.004804	1.423543	
0.001000	0.033135	0.016004	99.920000	99.480000	0.004765	1.418481	
0.001000	0.032870	0.016202	99.923333	99.470000	0.004671	1.409929	
0.001000	0.032830	0.016055	99.926667	99.490000	0.004658	1.408589	
0.001000	0.032642	0.016143	99.925000	99.510000	0.004635	1.400359	
0.001000	0.032928	0.016209	99.941667	99.460000	0.004563	1.418211	
0.001000	0.032702	0.016283	99.930000	99.430000	0.004479	1.411129	
0.001000	0.032500	0.016380	99.928333	99.450000	0.004487	1.400624	
0.001000	0.033064	0.016326	99.933333	99.410000	0.004461	1.430147	
{'adv_only': False, 'checkpoint': None, 'ckpt_interval': 10, 'dataset': 'MNIST', 'debug_mode': False, 'drop': 0, 'epochs': 100, 'epsilon': 0.3, 'evaluate': False, 'gamma': 0.1, 'gpu_id': '2', 'lambada': 0.02, 'lr': 0.001, 'manualSeed': 1000, 'method': 'LFM', 'momentum': 0.9, 'mu': 1.0, 'norm': 'l_inf', 'num_steps': 40, 'opt_g_gamma': 0.1, 'opt_g_lr': 0.1, 'opt_g_schedule': [100, 150, 200], 'opt_g_weight_decay': 0.0002, 'resume': '', 'save_dir': '0503dc_mnist', 'schedule': [90], 'start_epoch': 0, 'step_size': 0.01, 'test_batch': 128, 'train_batch': 128, 'weight_decay': 0, 'workers': 4}

Best acc: 
99.51
